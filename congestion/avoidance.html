

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6.4 Advanced Congestion Control &mdash; Computer Networks: A Systems Approach Version 6.1-dev documentation</title>
  

  
  
    <link rel="shortcut icon" href="../static/bridge.ico"/>
  
  
  

  
  <script type="text/javascript" src="../static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
        <script type="text/javascript" src="../static/jquery.js"></script>
        <script type="text/javascript" src="../static/underscore.js"></script>
        <script type="text/javascript" src="../static/doctools.js"></script>
        <script type="text/javascript" src="../static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../static/css/rtd_theme_mods.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6.5 Quality of Service" href="qos.html" />
    <link rel="prev" title="6.3 TCP Congestion Control" href="tcpcc.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Computer Networks: A Systems Approach
          

          
          </a>

          
            
            
              <div class="version">
                Version 6.1-dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foundation.html">Chapter 1:  Foundation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../direct.html">Chapter 2:  Direct Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internetworking.html">Chapter 3:  Internetworking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scaling.html">Chapter 4:  Advanced Internetworking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../e2e.html">Chapter 5:  End-to-End Protocols</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../congestion.html">Chapter 6:  Congestion Control</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="problem.html">Problem: Allocating Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="issues.html">6.1 Issues in Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="queuing.html">6.2 Queuing Disciplines</a></li>
<li class="toctree-l2"><a class="reference internal" href="tcpcc.html">6.3 TCP Congestion Control</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">6.4 Advanced Congestion Control</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#active-queue-management-decbit-red-ecn">Active Queue Management (DECbit, RED, ECN)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#decbit">DECbit</a></li>
<li class="toctree-l4"><a class="reference internal" href="#random-early-detection">Random Early Detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#explicit-congestion-notification">Explicit Congestion Notification</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#source-based-approaches-vegas-bbr-dctcp">Source-Based Approaches (Vegas, BBR, DCTCP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tcp-vegas">TCP Vegas</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tcp-bbr">TCP BBR</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dctcp">DCTCP</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="qos.html">6.5 Quality of Service</a></li>
<li class="toctree-l2"><a class="reference internal" href="trend.html">Perspective: Software-Defined Traffic Engineering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Chapter 7: End-to-End Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../security.html">Chapter 8: Network Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applications.html">Chapter 9: Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">About This Book</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Computer Networks: A Systems Approach</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../congestion.html">Chapter 6:  Congestion Control</a> &raquo;</li>
        
      <li>6.4 Advanced Congestion Control</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/congestion/avoidance.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="advanced-congestion-control">
<h1>6.4 Advanced Congestion Control<a class="headerlink" href="#advanced-congestion-control" title="Permalink to this headline">¶</a></h1>
<p>This section explores congestion control more deeply. In doing so, it is
important to understand that the standard TCP’s strategy is to control
congestion once it happens, as opposed to trying to avoid congestion in
the first place. In fact, TCP repeatedly increases the load it imposes
on the network in an effort to find the point at which congestion
occurs, and then it backs off from this point. Said another way, TCP
<em>needs</em> to create losses to find the available bandwidth of the
connection. An appealing alternative is to predict when congestion is
about to happen and then to reduce the rate at which hosts send data
just before packets start being discarded. We call such a strategy
<em>congestion avoidance</em> to distinguish it from <em>congestion control</em>, but
it’s probably most accurate to think of “avoidance” as a subset of
“control.”</p>
<p>We describe two different approaches to congestion-avoidance. The first
puts a small amount of additional functionality into the router to
assist the end node in the anticipation of congestion. This approach is
often referred to as <em>Active Queue Management</em> (AQM). The second
approach attempts to avoid congestion purely from the end hosts. This
approach is implemented in TCP, making it variant of the congestion
control mechanisms described in the previous section.</p>
<div class="section" id="active-queue-management-decbit-red-ecn">
<h2>Active Queue Management (DECbit, RED, ECN)<a class="headerlink" href="#active-queue-management-decbit-red-ecn" title="Permalink to this headline">¶</a></h2>
<p>The first approach requires changes to routers, which has never been the
Internet’s preferred way of introducing new features, but nonetheless,
has been a constant source of consternation over the last 20 years. The
problem is that while it’s generally agreed that routers are in an ideal
position to detect the onset of congestion—i.e., their queues start to
fill up—there has not been a consensus on exactly what the best
algorithm is. The following describes two of the classic mechanisms, and
concludes with a brief discussion of where things stand today.</p>
<div class="section" id="decbit">
<h3>DECbit<a class="headerlink" href="#decbit" title="Permalink to this headline">¶</a></h3>
<p>The first mechanism was developed for use on the Digital Network
Architecture (DNA), a connectionless network with a connection-oriented
transport protocol. This mechanism could, therefore, also be applied to
TCP and IP. As noted above, the idea here is to more evenly split the
responsibility for congestion control between the routers and the end
nodes. Each router monitors the load it is experiencing and explicitly
notifies the end nodes when congestion is about to occur. This
notification is implemented by setting a binary congestion bit in the
packets that flow through the router, hence the name <em>DECbit</em>. The
destination host then copies this congestion bit into the ACK it sends
back to the source. Finally, the source adjusts its sending rate so as
to avoid congestion. The following discussion describes the algorithm in
more detail, starting with what happens in the router.</p>
<p>A single congestion bit is added to the packet header. A router sets
this bit in a packet if its average queue length is greater than or
equal to&nbsp;1 at the time the packet arrives. This average queue length
is measured over a time interval that spans the last busy+idle cycle,
plus the current busy cycle. (The router is <em>busy</em> when it is
transmitting and <em>idle</em> when it is not.) <a class="reference internal" href="#fig-decbit"><span class="std std-numref">Figure 166</span></a> shows the queue length at a router as a function of
time. Essentially, the router calculates the area under the curve and
divides this value by the time interval to compute the average queue
length. Using a queue length of&nbsp;1 as the trigger for setting the
congestion bit is a trade-off between significant queuing (and hence
higher throughput) and increased idle time (and hence lower delay). In
other words, a queue length of&nbsp;1 seems to optimize the power function.</p>
<div class="figure align-center" id="id1">
<span id="fig-decbit"></span><a class="reference internal image-reference" href="../_images/f06-14-9780123850591.png"><img alt="../_images/f06-14-9780123850591.png" src="../_images/f06-14-9780123850591.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Figure 166. </span><span class="caption-text">Computing average queue length at a router.</span></p>
</div>
<p>Now turning our attention to the host half of the mechanism, the source
records how many of its packets resulted in some router setting the
congestion bit. In particular, the source maintains a congestion window,
just as in TCP, and watches to see what fraction of the last window’s
worth of packets resulted in the bit being set. If less than 50% of the
packets had the bit set, then the source increases its congestion window
by one packet. If 50% or more of the last window’s worth of packets had
the congestion bit set, then the source decreases its congestion window
to 0.875 times the previous value. The value 50% was chosen as the
threshold based on analysis that showed it to correspond to the peak of
the power curve. The “increase by&nbsp;1, decrease by 0.875” rule was
selected because additive increase/multiplicative decrease makes the
mechanism stable.</p>
</div>
<div class="section" id="random-early-detection">
<h3>Random Early Detection<a class="headerlink" href="#random-early-detection" title="Permalink to this headline">¶</a></h3>
<p>A second mechanism, called <em>random early detection</em> (RED), is similar to
the DECbit scheme in that each router is programmed to monitor its own
queue length and, when it detects that congestion is imminent, to notify
the source to adjust its congestion window. RED, invented by Sally Floyd
and Van Jacobson in the early 1990s, differs from the DECbit scheme in
two major ways.</p>
<p>The first is that rather than explicitly sending a congestion
notification message to the source, RED is most commonly implemented
such that it <em>implicitly</em> notifies the source of congestion by dropping
one of its packets. The source is, therefore, effectively notified by
the subsequent timeout or duplicate ACK. In case you haven’t already
guessed, RED is designed to be used in conjunction with TCP, which
currently detects congestion by means of timeouts (or some other means
of detecting packet loss such as duplicate ACKs). As the “early” part of
the RED acronym suggests, the gateway drops the packet earlier than it
would have to, so as to notify the source that it should decrease its
congestion window sooner than it would normally have. In other words,
the router drops a few packets before it has exhausted its buffer space
completely, so as to cause the source to slow down, with the hope that
this will mean it does not have to drop lots of packets later on.</p>
<p>The second difference between RED and DECbit is in the details of how
RED decides when to drop a packet and what packet it decides to drop. To
understand the basic idea, consider a simple FIFO queue. Rather than
wait for the queue to become completely full and then be forced to drop
each arriving packet (the tail drop policy of the previous section), we
could decide to drop each arriving packet with some <em>drop probability</em>
whenever the queue length exceeds some <em>drop level</em>. This idea is called
<em>early random drop</em>. The RED algorithm defines the details of how to
monitor the queue length and when to drop a packet.</p>
<p>In the following paragraphs, we describe the RED algorithm as originally
proposed by Floyd and Jacobson. We note that several modifications have
since been proposed both by the inventors and by other researchers.
However, the key ideas are the same as those presented below, and most
current implementations are close to the algorithm that follows.</p>
<p>First, RED computes an average queue length using a weighted running
average similar to the one used in the original TCP timeout computation.
That is, <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> is computed as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AvgLen</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Weight</span><span class="p">)</span> <span class="n">x</span> <span class="n">AvgLen</span> <span class="o">+</span> <span class="n">Weight</span> <span class="n">x</span> <span class="n">SampleLen</span>
</pre></div>
</div>
<p>where 0 &lt; <code class="docutils literal notranslate"><span class="pre">Weight</span></code> &lt; 1 and <code class="docutils literal notranslate"><span class="pre">SampleLen</span></code> is the length of the queue
when a sample measurement is made. In most software implementations, the
queue length is measured every time a new packet arrives at the gateway.
In hardware, it might be calculated at some fixed sampling interval.</p>
<p>The reason for using an average queue length rather than an
instantaneous one is that it more accurately captures the notion of
congestion. Because of the bursty nature of Internet traffic, queues
can become full very quickly and then become empty again. If a queue
is spending most of its time empty, then it’s probably not appropriate
to conclude that the router is congested and to tell the hosts to slow
down. Thus, the weighted running average calculation tries to detect
long-lived congestion, as indicated in the right-hand portion of
<a class="reference internal" href="#fig-red-avg"><span class="std std-numref">Figure 167</span></a>, by filtering out short-term changes
in the queue length. You can think of the running average as a
low-pass filter, where <code class="docutils literal notranslate"><span class="pre">Weight</span></code> determines the time constant of the
filter. The question of how we pick this time constant is discussed
below.</p>
<div class="figure align-center" id="id2">
<span id="fig-red-avg"></span><a class="reference internal image-reference" href="../_images/f06-15-9780123850591.png"><img alt="../_images/f06-15-9780123850591.png" src="../_images/f06-15-9780123850591.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Figure 167. </span><span class="caption-text">Weighted running average queue length.</span></p>
</div>
<p>Second, RED has two queue length thresholds that trigger certain
activity: <code class="docutils literal notranslate"><span class="pre">MinThreshold</span></code> and <code class="docutils literal notranslate"><span class="pre">MaxThreshold</span></code>. When a packet arrives
at the gateway, RED compares the current <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> with these two
thresholds, according to the following rules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">AvgLen</span> <span class="o">&lt;=</span> <span class="n">MinThreshold</span>
    <span class="n">queue</span> <span class="n">the</span> <span class="n">packet</span>
<span class="k">if</span> <span class="n">MinThreshold</span> <span class="o">&lt;</span> <span class="n">AvgLen</span> <span class="o">&lt;</span> <span class="n">MaxThreshold</span>
    <span class="n">calculate</span> <span class="n">probability</span> <span class="n">P</span>
    <span class="n">drop</span> <span class="n">the</span> <span class="n">arriving</span> <span class="n">packet</span> <span class="k">with</span> <span class="n">probability</span> <span class="n">P</span>
<span class="k">if</span> <span class="n">MaxThreshold</span> <span class="o">&lt;=</span> <span class="n">AvgLen</span>
    <span class="n">drop</span> <span class="n">the</span> <span class="n">arriving</span> <span class="n">packet</span>
</pre></div>
</div>
<p>If the average queue length is smaller than the lower threshold, no
action is taken, and if the average queue length is larger than the
upper threshold, then the packet is always dropped. If the average
queue length is between the two thresholds, then the newly arriving
packet is dropped with some probability <code class="docutils literal notranslate"><span class="pre">P</span></code>. This situation is
depicted in <a class="reference internal" href="#fig-red"><span class="std std-numref">Figure 168</span></a>. The approximate
relationship between <code class="docutils literal notranslate"><span class="pre">P</span></code> and <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> is shown in <a class="reference internal" href="#fig-red-prob"><span class="std std-numref">Figure
169</span></a>. Note that the probability of drop increases slowly
when <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> is between the two thresholds, reaching <code class="docutils literal notranslate"><span class="pre">MaxP</span></code> at
the upper threshold, at which point it jumps to unity. The rationale
behind this is that, if <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> reaches the upper threshold, then
the gentle approach (dropping a few packets) is not working and
drastic measures are called for: dropping all arriving packets. Some
research has suggested that a smoother transition from random dropping
to complete dropping, rather than the discontinuous approach shown
here, may be appropriate.</p>
<div class="figure align-center" id="id3">
<span id="fig-red"></span><a class="reference internal image-reference" href="../_images/f06-16-9780123850591.png"><img alt="../_images/f06-16-9780123850591.png" src="../_images/f06-16-9780123850591.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-number">Figure 168. </span><span class="caption-text">RED thresholds on a FIFO queue.</span></p>
</div>
<div class="figure align-center" id="id4">
<span id="fig-red-prob"></span><a class="reference internal image-reference" href="../_images/f06-17-9780123850591.png"><img alt="../_images/f06-17-9780123850591.png" src="../_images/f06-17-9780123850591.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Figure 169. </span><span class="caption-text">Drop probability function for RED.</span></p>
</div>
<p>Although <a class="reference internal" href="#fig-red-prob"><span class="std std-numref">Figure 169</span></a> shows the probability of
drop as a function only of <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code>, the situation is actually a
little more complicated. In fact, <code class="docutils literal notranslate"><span class="pre">P</span></code> is a function of both
<code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> and how long it has been since the last packet was
dropped. Specifically, it is computed as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TempP</span> <span class="o">=</span> <span class="n">MaxP</span> <span class="n">x</span> <span class="p">(</span><span class="n">AvgLen</span> <span class="o">-</span> <span class="n">MinThreshold</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">MaxThreshold</span> <span class="o">-</span> <span class="n">MinThreshold</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">TempP</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">count</span> <span class="n">x</span> <span class="n">TempP</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">TempP</span></code> is the variable that is plotted on the y-axis in <a class="reference internal" href="#fig-red-prob"><span class="std std-numref">Figure
169</span></a>, <code class="docutils literal notranslate"><span class="pre">count</span></code> keeps track of how many newly arriving
packets have been queued (not dropped), and <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> has been between
the two thresholds. <code class="docutils literal notranslate"><span class="pre">P</span></code> increases slowly as <code class="docutils literal notranslate"><span class="pre">count</span></code> increases,
thereby making a drop increasingly likely as the time since the last
drop increases. This makes closely spaced drops relatively less likely
than widely spaced drops. This extra step in calculating <code class="docutils literal notranslate"><span class="pre">P</span></code> was
introduced by the inventors of RED when they observed that, without it,
the packet drops were not well distributed in time but instead tended to
occur in clusters. Because packet arrivals from a certain connection are
likely to arrive in bursts, this clustering of drops is likely to cause
multiple drops in a single connection. This is not desirable, since only
one drop per round-trip time is enough to cause a connection to reduce
its window size, whereas multiple drops might send it back into slow
start.</p>
<p>As an example, suppose that we set <code class="docutils literal notranslate"><span class="pre">MaxP</span></code> to 0.02 and <code class="docutils literal notranslate"><span class="pre">count</span></code> is
initialized to zero. If the average queue length were halfway between
the two thresholds, then <code class="docutils literal notranslate"><span class="pre">TempP</span></code>, and the initial value of <code class="docutils literal notranslate"><span class="pre">P</span></code>,
would be half of <code class="docutils literal notranslate"><span class="pre">MaxP</span></code>, or 0.01. An arriving packet, of course, has a
99 in 100 chance of getting into the queue at this point. With each
successive packet that is not dropped, <code class="docutils literal notranslate"><span class="pre">P</span></code> slowly increases, and by
the time 50&nbsp;packets have arrived without a drop, <code class="docutils literal notranslate"><span class="pre">P</span></code> would have
doubled to 0.02. In the unlikely event that 99 packets arrived without
loss, <code class="docutils literal notranslate"><span class="pre">P</span></code> reaches&nbsp;1, guaranteeing that the next packet is dropped. The
important thing about this part of the algorithm is that it ensures a
roughly even distribution of drops over time.</p>
<p>The intent is that, if RED drops a small percentage of packets when
<code class="docutils literal notranslate"><span class="pre">AvgLen</span></code> exceeds <code class="docutils literal notranslate"><span class="pre">MinThreshold</span></code>, this will cause a few TCP
connections to reduce their window sizes, which in turn will reduce the
rate at which packets arrive at the router. All going well, <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code>
will then decrease and congestion is avoided. The queue length can be
kept short, while throughput remains high since few packets are dropped.</p>
<p>Note that, because RED is operating on a queue length averaged over
time, it is possible for the instantaneous queue length to be much
longer than <code class="docutils literal notranslate"><span class="pre">AvgLen</span></code>. In this case, if a packet arrives and there is
nowhere to put it, then it will have to be dropped. When this happens,
RED is operating in tail drop mode. One of the goals of RED is to
prevent tail drop behavior if possible.</p>
<p>The random nature of RED confers an interesting property on the
algorithm. Because RED drops packets randomly, the probability that RED
decides to drop a particular flow’s packet(s) is roughly proportional to
the share of the bandwidth that that flow is currently getting at that
router. This is because a flow that is sending a relatively large number
of packets is providing more candidates for random dropping. Thus, there
is some sense of fair resource allocation built into RED, although it is
by no means precise. While arguably fair, because RED punishes
high-bandwidth flows more than low-bandwidth flows, it increases the
probability of a TCP restart, which is doubly painful for those
high-bandwidth flows.</p>
<div class="admonition-key-takeaway admonition">
<p class="first admonition-title">Key Takeaway</p>
<p class="last">Note that a fair amount of analysis has gone into setting the various
RED parameters—for example, and <code class="docutils literal notranslate"><span class="pre">Weight</span></code>—all in the name of
optimizing the power function (throughput-to-delay ratio). The
performance of these parameters has also been confirmed through
simulation, and the algorithm has been shown not to be overly
sensitive to them. It is important to keep in mind, however, that all
of this analysis and simulation hinges on a particular
characterization of the network workload. The real contribution of
RED is a mechanism by which the router can more accurately manage its
queue length. Defining precisely what constitutes an optimal queue
length depends on the traffic mix and is still a subject of research,
with real information now being gathered from operational deployment
of RED in the Internet.</p>
</div>
<p>Consider the setting of the two thresholds, <code class="docutils literal notranslate"><span class="pre">MinThreshold</span></code> and
<code class="docutils literal notranslate"><span class="pre">MaxThreshold</span></code>. If the traffic is fairly bursty, then <code class="docutils literal notranslate"><span class="pre">MinThreshold</span></code>
should be sufficiently large to allow the link utilization to be
maintained at an acceptably high level. Also, the difference between the
two thresholds should be larger than the typical increase in the
calculated average queue length in one RTT. Setting <code class="docutils literal notranslate"><span class="pre">MaxThreshold</span></code> to
twice <code class="docutils literal notranslate"><span class="pre">MinThreshold</span></code> seems to be a reasonable rule of thumb given the
traffic mix on today’s Internet. In addition, since we expect the
average queue length to hover between the two thresholds during periods
of high load, there should be enough free buffer space <em>above</em>
<code class="docutils literal notranslate"><span class="pre">MaxThreshold</span></code> to absorb the natural bursts that occur in Internet
traffic without forcing the router to enter tail drop mode.</p>
<p>We noted above that <code class="docutils literal notranslate"><span class="pre">Weight</span></code> determines the time constant for the
running average low-pass filter, and this gives us a clue as to how we
might pick a suitable value for it. Recall that RED is trying to send
signals to TCP flows by dropping packets during times of congestion.
Suppose that a router drops a packet from some TCP connection and then
immediately forwards some more packets from the same connection. When
those packets arrive at the receiver, it starts sending duplicate ACKs
to the sender. When the sender sees enough duplicate ACKs, it will
reduce its window size. So, from the time the router drops a packet
until the time when the same router starts to see some relief from the
affected connection in terms of a reduced window size, at least one
round-trip time must elapse for that connection. There is probably not
much point in having the router respond to congestion on time scales
much less than the round-trip time of the connections passing through
it. As noted previously, 100&nbsp;ms is not a bad estimate of average
round-trip times in the Internet. Thus, <code class="docutils literal notranslate"><span class="pre">Weight</span></code> should be chosen such
that changes in queue length over time scales much less than 100&nbsp;ms are
filtered out.</p>
<p>Since RED works by sending signals to TCP flows to tell them to slow
down, you might wonder what would happen if those signals are ignored.
This is often called the <em>unresponsive flow</em> problem. Unresponsive flows
use more than their fair share of network resources and could cause
congestive collapse if there were enough of them, just as in the days
before TCP congestion control. Some of the techniques described in the
next section can help with this problem by isolating certain classes of
traffic from others. There is also the possibility that a variant of RED
could drop more heavily from flows that are unresponsive to the initial
hints that it sends.</p>
</div>
<div class="section" id="explicit-congestion-notification">
<h3>Explicit Congestion Notification<a class="headerlink" href="#explicit-congestion-notification" title="Permalink to this headline">¶</a></h3>
<p>RED is the most extensively studied AQM mechanism, but it has not been
widely deployed, due in part to the fact that it does not result in
ideal behavior in all circumstances. It is, however, the benchmark for
understanding AQM behavior. The other thing that came out of RED is the
recognition that TCP could do a better job if routers were to send a
more explicit congestion signal.</p>
<p>That is, instead of <em>dropping</em> a packet and assuming TCP will eventually
notice (e.g., due to the arrival of a duplicate ACK), RED (or any AQM
algorithm for that matter) can do a better job if it instead <em>marks</em> the
packet and continues to send it along its way to the destination. This
idea was codified in changes to the IP and TCP headers known as
<em>Explicit Congestion Notification</em> (ECN).</p>
<p>Specifically, this feedback is implemented by treating two bits in the
IP <code class="docutils literal notranslate"><span class="pre">TOS</span></code> field as ECN bits. One bit is set by the source to indicate
that it is ECN-capable, that is, able to react to a congestion
notification. This is called the <code class="docutils literal notranslate"><span class="pre">ECT</span></code> bit (ECN-Capable Transport).
The other bit is set by routers along the end-to-end path when
congestion is encountered, as computed by whatever AQM algorithm it is
running. This is called the <code class="docutils literal notranslate"><span class="pre">CE</span></code> bit (Congestion Encountered).</p>
<p>In addition to these two bits in the IP header (which are
transport-agnostic), ECN also includes the addition of two optional
flags to the TCP header. The first, <code class="docutils literal notranslate"><span class="pre">ECE</span></code> (ECN-Echo), communicates
from the receiver to the sender that it has received a packet with the
<code class="docutils literal notranslate"><span class="pre">CE</span></code> bit set. The second, <code class="docutils literal notranslate"><span class="pre">CWR</span></code> (Congestion Window Reduced)
communicates from the sender to the receiver that it has reduced the
congestion window.</p>
<p>While ECN is now the standard interpretation of two of the eight bits in
the <code class="docutils literal notranslate"><span class="pre">TOS</span></code> field of the IP header and support for ECN is highly
recommended, it is not required. Moreover, there is no single
recommended AQM algorithm, but instead, there is a list of requirements
a good AQM algorithm should meet. Like TCP congestion control
algorithms, every AQM algorithm has its advantages and disadvantages,
and so we need a lot of them. There is one particular scenario, however,
where the TCP congestion control algorithm and AQM algorithm are
designed to work in concert: the datacenter. We return to this use case
at the end of this section.</p>
</div>
</div>
<div class="section" id="source-based-approaches-vegas-bbr-dctcp">
<h2>Source-Based Approaches (Vegas, BBR, DCTCP)<a class="headerlink" href="#source-based-approaches-vegas-bbr-dctcp" title="Permalink to this headline">¶</a></h2>
<p>Unlike the previous congestion-avoidance schemes, which depended on
cooperation from routers, we now describe a strategy for detecting the
incipient stages of congestion—before losses occur—from the end hosts.
We first give a brief overview of a collection of related mechanisms
that use different information to detect the early stages of congestion,
and then we describe two specific mechanisms in more detail.</p>
<p>The general idea of these techniques is to watch for a sign from the
network that some router’s queue is building up and that congestion will
happen soon if nothing is done about it. For example, the source might
notice that as packet queues build up in the network’s routers, there is
a measurable increase in the RTT for each successive packet it sends.
One particular algorithm exploits this observation as follows: The
congestion window normally increases as in TCP, but every two round-trip
delays the algorithm checks to see if the current RTT is greater than
the average of the minimum and maximum RTTs seen so far. If it is, then
the algorithm decreases the congestion window by one-eighth.</p>
<p>A second algorithm does something similar. The decision as to whether or
not to change the current window size is based on changes to both the
RTT and the window size. The window is adjusted once every two
round-trip delays based on the product</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">CurrentWindow</span> <span class="o">-</span> <span class="n">OldWindow</span><span class="p">)</span> <span class="n">x</span> <span class="p">(</span><span class="n">CurrentRTT</span> <span class="o">-</span> <span class="n">OldRTT</span><span class="p">)</span>
</pre></div>
</div>
<p>If the result is positive, the source decreases the window size by
one-eighth; if the result is negative or&nbsp;0, the source increases the
window by one maximum packet size. Note that the window changes during
every adjustment; that is, it oscillates around its optimal point.</p>
<p>Another change seen as the network approaches congestion is the
flattening of the sending rate. A third scheme takes advantage of this
fact. Every RTT, it increases the window size by one packet and compares
the throughput achieved to the throughput when the window was one packet
smaller. If the difference is less than one-half the throughput achieved
when only one packet was in transit—as was the case at the beginning of
the connection—the algorithm decreases the window by one packet. This
scheme calculates the throughput by dividing the number of bytes
outstanding in the network by the RTT.</p>
<div class="section" id="tcp-vegas">
<h3>TCP Vegas<a class="headerlink" href="#tcp-vegas" title="Permalink to this headline">¶</a></h3>
<p>The mechanism we are going to describe in more detail is similar to the
last algorithm in that it looks at changes in the throughput rate or,
more specifically, changes in the sending rate. However, it differs from
the previous algorithm in the way it calculates throughput, and instead
of looking for a change in the slope of the throughput it compares the
measured throughput rate with an expected throughput rate. The
algorithm, TCP Vegas, is not widely deployed in the Internet today, but
the strategy it uses has been adopted by other implementations that are
now being deployed.</p>
<p>The intuition behind the Vegas algorithm can be seen in the trace of
standard TCP given in <a class="reference internal" href="#fig-trace3"><span class="std std-numref">Figure 170</span></a>. The top graph
shown in <a class="reference internal" href="#fig-trace3"><span class="std std-numref">Figure 170</span></a> traces the connection’s
congestion window; it shows the same information as the traces given
earlier in this section.  The middle and bottom graphs depict new
information: The middle graph shows the average sending rate as
measured at the source, and the bottom graph shows the average queue
length as measured at the bottleneck router. All three graphs are
synchronized in time. In the period between 4.5 and 6.0&nbsp;seconds
(shaded region), the congestion window increases (top graph). We
expect the observed throughput to also increase, but instead it stays
flat (middle graph). This is because the throughput cannot increase
beyond the available bandwidth. Beyond this point, any increase in the
window size only results in packets taking up buffer space at the
bottleneck router (bottom graph).</p>
<div class="figure align-center" id="id5">
<span id="fig-trace3"></span><a class="reference internal image-reference" href="../_images/f06-18-9780123850591.png"><img alt="../_images/f06-18-9780123850591.png" src="../_images/f06-18-9780123850591.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Figure 170. </span><span class="caption-text">Congestion window versus observed throughput rate (the
three graphs are synchronized). Top, congestion window; middle,
observed throughput; bottom, buffer space taken up at the
router. Colored line = <cite>CongestionWindow</cite>; solid bullet = timeout;
hash marks = time when each packet is transmitted; vertical bars =
time when a packet that was eventually retransmitted was first
transmitted.</span></p>
</div>
<p>A useful metaphor that describes the phenomenon illustrated in
<a class="reference internal" href="#fig-trace3"><span class="std std-numref">Figure 170</span></a> is driving on ice. The speedometer
(congestion window) may say that you are going 30 miles an hour, but
by looking out the car window and seeing people pass you on foot
(measured sending rate) you know that you are going no more than 5
miles an hour. The extra energy is being absorbed by the car’s tires
(router buffers).</p>
<p>TCP Vegas uses this idea to measure and control the amount of extra data
this connection has in transit, where by “extra data” we mean data that
the source would not have transmitted had it been trying to match
exactly the available bandwidth of the network. The goal of TCP Vegas is
to maintain the “right” amount of extra data in the network. Obviously,
if a source is sending too much extra data, it will cause long delays
and possibly lead to congestion. Less obviously, if a connection is
sending too little extra data, it cannot respond rapidly enough to
transient increases in the available network bandwidth. TCP Vegas’s
congestion-avoidance actions are based on changes in the estimated
amount of extra data in the network, not only on dropped packets. We now
describe the algorithm in detail.</p>
<p>First, define a given flow’s <code class="docutils literal notranslate"><span class="pre">BaseRTT</span></code> to be the RTT of a packet when
the flow is not congested. In practice, TCP Vegas sets <code class="docutils literal notranslate"><span class="pre">BaseRTT</span></code> to
the minimum of all measured round-trip times; it is commonly the RTT of
the first packet sent by the connection, before the router queues
increase due to traffic generated by this flow. If we assume that we are
not overflowing the connection, then the expected throughput is given by</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ExpectedRate</span> <span class="o">=</span> <span class="n">CongestionWindow</span> <span class="o">/</span> <span class="n">BaseRTT</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">CongestionWindow</span></code> is the TCP congestion window, which we
assume (for the purpose of this discussion) to be equal to the number
of bytes in transit.</p>
<p>Second, TCP Vegas calculates the current sending rate, <code class="docutils literal notranslate"><span class="pre">ActualRate</span></code>.
This is done by recording the sending time for a distinguished packet,
recording how many bytes are transmitted between the time that packet
is sent and when its acknowledgment is received, computing the sample
RTT for the distinguished packet when its acknowledgment arrives, and
dividing the number of bytes transmitted by the sample RTT. This
calculation is done once per round-trip time.</p>
<p>Third, TCP Vegas compares <code class="docutils literal notranslate"><span class="pre">ActualRate</span></code> to <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code> and
adjusts the window accordingly. We let <code class="docutils literal notranslate"><span class="pre">Diff</span> <span class="pre">=</span> <span class="pre">ExpectedRate</span> <span class="pre">-</span>
<span class="pre">ActualRate</span></code>.  Note that <code class="docutils literal notranslate"><span class="pre">Diff</span></code> is positive or&nbsp;0 by definition,
since <code class="docutils literal notranslate"><span class="pre">ActualRate</span> <span class="pre">&gt;ExpectedRate</span></code> implies that we need to change
<code class="docutils literal notranslate"><span class="pre">BaseRTT</span></code> to the latest sampled RTT. We also define two thresholds,
<em>α &lt; β</em>, roughly corresponding to having too little and too much extra
data in the network, respectively. When <code class="docutils literal notranslate"><span class="pre">Diff</span></code> &lt; <em>α</em>, TCP Vegas
increases the congestion window linearly during the next RTT, and when
<code class="docutils literal notranslate"><span class="pre">Diff</span></code> &gt; <em>β</em>, TCP Vegas decreases the congestion window linearly
during the next RTT.  TCP Vegas leaves the congestion window unchanged
when <em>α</em> &lt; <code class="docutils literal notranslate"><span class="pre">Diff</span></code> &lt; <em>β</em>.</p>
<p>Intuitively, we can see that the farther away the actual throughput
gets from the expected throughput, the more congestion there is in the
network, which implies that the sending rate should be reduced. The
<em>β</em> threshold triggers this decrease. On the other hand, when the
actual throughput rate gets too close to the expected throughput, the
connection is in danger of not utilizing the available bandwidth. The
<em>α</em> threshold triggers this increase. The overall goal is to keep
between<em>α</em> and <em>β</em> extra bytes in the network.</p>
<div class="figure align-center" id="id6">
<span id="fig-vegas"></span><a class="reference internal image-reference" href="../_images/f06-19-9780123850591.png"><img alt="../_images/f06-19-9780123850591.png" src="../_images/f06-19-9780123850591.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Figure 171. </span><span class="caption-text">Trace of TCP Vegas congestion-avoidance mechanism.
Top, congestion window; bottom, expected (colored line) and actual
(black line) throughput. The shaded area is the region between the
<em>α</em> and <em>β</em> thresholds.</span></p>
</div>
<p><a class="reference internal" href="#fig-vegas"><span class="std std-numref">Figure 171</span></a> traces the TCP Vegas
congestion-avoidance algorithm. The top graph traces the congestion
window, showing the same information as the other traces given
throughout this chapter. The bottom graph traces the expected and
actual throughput rates that govern how the congestion window is
set. It is this bottom graph that best illustrates how the algorithm
works. The colored line tracks the <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code>, while the black
line tracks the <code class="docutils literal notranslate"><span class="pre">ActualRate</span></code>. The wide shaded strip gives the region
between the <em>α</em> and <em>β</em> thresholds; the top of the shaded strip is
<em>α</em>&nbsp;KBps away from <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code>, and the bottom of the shaded
strip is <em>β</em>&nbsp;KBps away from <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code>.  The goal is to keep the
<code class="docutils literal notranslate"><span class="pre">ActualRate</span></code> between these two thresholds, within the shaded
region. Whenever <code class="docutils literal notranslate"><span class="pre">ActualRate</span></code> falls below the shaded region (i.e.,
gets too far from <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code>), TCP Vegas decreases the
congestion window because it fears that too many packets are being
buffered in the network. Likewise, whenever <code class="docutils literal notranslate"><span class="pre">ActualRate</span></code> goes above
the shaded region (i.e., gets too close to the <code class="docutils literal notranslate"><span class="pre">ExpectedRate</span></code>), TCP
Vegas increases the congestion window because it fears that it is
underutilizing the network.</p>
<p>Because the algorithm, as just presented, compares the difference
between the actual and expected throughput rates to the <em>α</em> and <em>β</em>
thresholds, these two thresholds are defined in terms of KBps. However,
it is perhaps more accurate to think in terms of how many extra
<em>buffers</em> the connection is occupying in the network. For example, on a
connection with a <code class="docutils literal notranslate"><span class="pre">BaseRTT</span></code> of 100&nbsp;ms and a packet size of 1&nbsp;KB, if
<em>α</em> = 30&nbsp;KBps and <em>β</em> = 60&nbsp;KBps, then we can think of <em>α</em> as specifying
that the connection needs to be occupying at least 3 extra buffers in
the network and <em>β</em> as specifying that the connection should occupy no
more than 6 extra buffers in the network. In practice, a setting of <em>α</em>
to 1 buffer and <em>β</em> to 3 buffers works well.</p>
<p>Finally, you will notice that TCP Vegas decreases the congestion window
linearly, seemingly in conflict with the rule that multiplicative
decrease is needed to ensure stability. The explanation is that TCP
Vegas does use multiplicative decrease when a timeout occurs; the linear
decrease just described is an <em>early</em> decrease in the congestion window
that should happen before congestion occurs and packets start being
dropped.</p>
</div>
<div class="section" id="tcp-bbr">
<h3>TCP BBR<a class="headerlink" href="#tcp-bbr" title="Permalink to this headline">¶</a></h3>
<p>BBR (Bottleneck Bandwidth and RTT) is a new TCP congestion control
algorithm developed by researchers at Google. Like Vegas, BBR is delay
based, which means it tries to detect buffer growth so as to avoid
congestion and packet loss. Both BBR and Vegas use the minimum RTT and
maximum RTT, as calculated over some time interval, as their main
control signals.</p>
<p>BBR also introduces new mechanisms to improve performance, including
packet pacing, bandwidth probing, and RTT probing. Packet pacing spaces
the packets based on the estimate of the available bandwidth. This
eliminates bursts and unnecessary queueing, which results in a better
feedback signal. BBR also periodically increases its rate, thereby
probing the available bandwidth. Similarly, BBR periodically decreases
its rate, thereby probing for a new minimum RTT. The RTT probing
mechanism attempts to be self-synchronizing, which is to say, when there
are multiple BBR flows, their respective RTT probes happen at the same
time. This gives a more accurate view of the actual uncongested path
RTT, which solves one of the major issues with delay-based congestion
control mechanisms: having accurate knowledge of the uncongested path
RTT.</p>
<p>BBR is actively being worked on and rapidly evolving. One major focus is
fairness. For example, some experiments show CUBIC flows get 100× less
bandwidth when competing with BBR flows, and other experiments show that
unfairness among BBR flows is even possible. Another major focus is
avoiding high retransmission rates, where in some cases as many as 10%
of packets are retransmitted.</p>
</div>
<div class="section" id="dctcp">
<h3>DCTCP<a class="headerlink" href="#dctcp" title="Permalink to this headline">¶</a></h3>
<p>We conclude with an example of a situation where a variant of the TCP
congestion control algorithm has been designed to work in concert with
ECN: in cloud datacenters. The combination is called DCTCP, which stands
for <em>Data Center TCP</em>. The situation is unique in that a datacenter is
self-contained, and so it is possible to deploy a tailor-made version of
TCP that does not need to worry about treating other TCP flows fairly.
Datacenters are also unique in that they are built using low-cost
white-box switches, and because there is no need to worry about long-fat
pipes spanning a continent, the switches are typically provisioned
without an excess of buffers.</p>
<p>The idea is straightforward. DCTCP adapts ECN by estimating the fraction
of bytes that encounter congestion rather than simply detecting that
some congestion is about to occur. At the end hosts, DCTCP then scales
the congestion window based on this estimate. The standard TCP algorithm
still kicks in should a packet actually be lost. The approach is
designed to achieve high-burst tolerance, low latency, and high
throughput with shallow-buffered switches.</p>
<p>The key challenge DCTCP faces is to estimate the fraction of bytes
encountering congestion. Each switch is simple. If a packet arrives and
the switch sees the queue length (K) is above some threshold; e.g.,</p>
<div class="math notranslate nohighlight">
\[\mathsf{K} &gt; (\mathsf{RTT} \times \mathsf{C})/7\]</div>
<p>where C is the link rate in packets per second, then the switch sets the
CE bit in the IP header. The complexity of RED is not required.</p>
<p>The receiver then maintains a boolean variable for every flow, which
we’ll denote <code class="docutils literal notranslate"><span class="pre">SeenCE</span></code>, and implements the following state machine in
response to every received packet:</p>
<ul class="simple">
<li>If the CE bit is set and <code class="docutils literal notranslate"><span class="pre">SeenCE=False</span></code>, set <code class="docutils literal notranslate"><span class="pre">SeenCE</span></code> to True and
send an immediate ACK.</li>
<li>If the CE bit is not set and <code class="docutils literal notranslate"><span class="pre">SeenCE=True</span></code>, set <code class="docutils literal notranslate"><span class="pre">SeenCE</span></code> to False
and send an immediate ACK.</li>
<li>Otherwise, ignore the CE bit.</li>
</ul>
<p>The non-obvious consequence of the “otherwise” case is that the receiver
continues to send delayed ACKs once every <em>n</em> packets, whether or not
the CE bit is set. This has proven important to maintaining high
performance.</p>
<p>Finally, the sender computes the fraction of bytes that encountered
congestion during the previous observation window (usually chosen to be
approximately the RTT), as the ratio of the total bytes transmitted and
the bytes acknowledged with the ECE flag set. DCTCP grows the congestion
window in exactly the same way as the standard algorithm, but it reduces
the window in proportion to how many bytes encountered congestion during
the last observation window.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="qos.html" class="btn btn-neutral float-right" title="6.5 Quality of Service" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tcpcc.html" class="btn btn-neutral float-left" title="6.3 TCP Congestion Control" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
