Perspective: Big Data and Analytics
===================================

This chapter is about data, and since no topic in Computer Science is
receiving more attention than *big data* (or alternatively, *data
analytics*), a natural question is what relationship there might be
between big data and computer networks. Although the term is often used
informally by the popular press, a working definition is quite simple:
sensor data is collected by monitoring some physical or man-made system
and then analyzed for insights using the statistical methods of Machine
Learning. Because the amount of raw data that’s collected is often
voluminous, the “big” qualifier is applied. So are there any
implications to networking?

At first blush, networks are purposely designed to be data-agnostic. If
you collect it and want to ship to somewhere for analysis, the network
is happy to do that for you. You might compress the data to reduce the
bandwidth required to transmit it, but otherwise big data is no
different than plain old regular data. But this ignores two important
factors.

The first is that while the network doesn’t care about the meaning of
the data (i.e., what the bits represent), it does concern itself with
the volume of data. This impacts the access network in particular, which
has been engineered to favor download speeds over upload speeds. That
bias makes sense when the dominant use case is video that flows out to
end-users, but in a world where your car, every appliance in your house,
and the drones flying over your city are all reporting data back into
the network (uploaded into the cloud), the situation is reversed. In
fact, the amount of data being generated by *Autonomous Vehicles* and
the *Internet-of-Things (IoT)* is potentially overwhelming.

While one could imagine dealing with this problem by using one of the
compression algorithms described in :ref:`Section 7.2 <7.2 Multimedia Data>`,
people are instead thinking outside the box, and pursuing new
applications that reside at the edge of the network. These *edge-native
applications* both provide better sub-millisecond response time and they
dramatically reduce the volume of data that ultimately needs to be
uploaded into the cloud. You can think of this data reduction as
application-specific compression, but it’s more accurate to say that the
edge application needs only write summaries of the data, not the raw
data, back to the cloud.

We introduced the access-edge cloud technology needed to support
edge-native applications at the end of :ref:`Chapter 2 <Perspective:
Race to the Edge>`, but what is perhaps more interesting is to
look at some examples of edge-native applications. One such example is
enterprises in the automotive, factory, and warehouse space increasingly
want to deploy private 5G networks for a variety of *physical
automation* use cases. These include a garage where a remote valet parks
your car or a factory floor making use of automation robots. The common
theme is high bandwidth, low latency connectivity from the robot to
intelligence sitting nearby in an edge cloud. This drives lower robot
costs (you don’t need to place heavy compute on each one) and enables
robot swarms and coordination more scalably.

Another illustrative example is *Wearable Cognitive Assistance*. The
idea is to generalize what navigation software does for us: it uses one
sensor (GPS), gives us step-by-step guidance on a complex task (getting
around an unknown city), catches our errors promptly, and helps us
recover. Can we generalize this metaphor? Could a person wearing a
device (e.g., Google Glass, Microsoft Hololens) be guided step-by-step
on a complex task, perhaps for the first time? The system would
effectively act as “an angel on your shoulder.” All the sensors on the
device (e.g., video, audio, accelerometer, gyroscope) are streamed over
wireless (possibly after some device preprocessing) to a nearby
edge-cloud that performs the heavy lifting. This is a human-in-the-loop
metaphor, with the “look and feel of augmented reality” but implemented
by AI algorithms (e.g., computer vision, natural language recognition.)

The second factor is that because a network is like many other man-made
systems, it is possible to collect data about its behavior (e.g.,
performance, failures, traffic patterns), apply analytics programs to
that data, and use the insights gained to improve the network. It should
not come as a surprise that this is an active area of research, with the
goal of building a closed control loop. Setting aside the analytics
itself, which are well outside the scope of this book, the interesting
questions are (1) what useful data can we collect, and (2) what aspects
of the network are most promising to control? Let’s look at two
promising answers.

One is 5G cellular networks, which are inherently complex. They include
multiple layers of virtual functions, virtual and physical RAN assets,
spectrum usage, and as we have just discussed, edge computing nodes. It
is widely expected that network analytics will be essential to building
a flexible 5G network. This will include network planning, which will
need to decide where to scale specific network functions and application
services based on machine learning algorithms that analyze network
utilization and traffic data patterns.

A second is *In-band Network Telemetry* (INT), a framework to collect
and report network state, directly in the data plane. This is in
contrast to the conventional reporting done by the network control
plane, as typified by the example systems described in :ref:`Section
9.3 <9.3 Infrastructure Applications>`. In the INT architecture,
packets contain header fields that are interpreted as “telemetry
instructions” by network devices. These instructions tell an INT-capable
device what state to collect and write into the packet as it transits
the network. INT *traffic sources* (e.g., applications, end-host
networking stacks, VM hypervisors) can embed the instructions either in
normal data packets or in special probe packets. Similarly, INT *traffic
sinks* retrieve (and optionally report) the collected results of these
instructions, allowing the traffic sinks to monitor the exact data plane
state that the packets “observed” while being forwarded. INT is still
early-stage, and takes advantage of the programmable pipelines described
in :ref:`Section 3.5 <3.5 Implementation>`, but it has the
potential to provide a qualitatively deeper insights into traffic
patterns and the root causes of network failures.

.. admonition::  Broader Perspective

   To continue reading about the cloudification of the Internet, see
   :ref:`Perspective: Blockchain and a Decentralized Internet`.

   To learn more about promising edge-native applications, we recommend:
   `Open Edge Computing Initiative <http://openedgecomputing.org>`__,
   2019.

   To learn more about In-band Network Telemetry, we recommend:
   `In-band Network Telemetry via Programmable
   Dataplanes <https://pdfs.semanticscholar.org/a3f1/9dc8520e2f42673be7cbd8d80cd96e3ec0c1.pdf>`__,
   August 2015.
